{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_Word2Vec(CNN,BiLstm).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"U28R4viCXx6E"},"source":["import gensim\n","import numpy as np\n","import nltk\n","import pandas as pd \n","from tqdm import tqdm\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional, Input, GlobalAveragePooling1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import Sequential\n","import nltk\n","from nltk.stem.lancaster import LancasterStemmer\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model, to_categorical\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import tensorflow as tf\n","from nltk.stem import WordNetLemmatizer\n","from keras_preprocessing.text import Tokenizer\n","from sklearn.metrics import accuracy_score, log_loss \n","from sklearn.model_selection import StratifiedKFold\n","from nltk.corpus import stopwords \n","import re \n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OwOGTGFJWGjP"},"source":["# **데이터 전처리**"]},{"cell_type":"code","metadata":{"id":"Tkm0qgOYWIM2"},"source":["#데이터 불러오기\n","\n","train_dataset = pd.read_csv(\"/content/train.csv\",encoding='utf-8') \n","test_dataset = pd.read_csv(\"/content/test_x.csv\",encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6XbLBvFWJ1O"},"source":["#데이터 중에 불어를 영어로 번역하기\n","\n","#번역해 줄 dictionary\n","translate_user_dict = {\"_un doigt d'eau de vie_.\":\"a finger of brandy\", \"Oui, j'ai pris un mot pour un autre.\":\"Yes, I took one word for another.\", \"Mais c'est égal.\":\"But it doesn't matter.\", \"Oui, j'ai beaucoup à vous dire, chère amie.\":\"Yes, I have a lot to tell you, dear friend.\", \"parce que nous avons à parler.\":\"because we have to talk.\", \"Pardon, j'ai oublié son nom.\":\"Sorry, I forgot his name.\", \"Il n'est pas du pays\":\"He is not from the country\", \"quelque chose de bête et d'Allemand dans la physionomie.\":\"something stupid and German in the physiognomy.\", \"C'est encore mieux\":\"It's even better\", \"j'ai en tout quarante roubles mais\":\"I have forty rubles in all but\", \"Grace à Dieu\":\"Thanks to God\", \"c'est une si pauvre tête!\":\"that is such a poor head!\", \"c'est un pauvre sire, tout de même\":\"he is a poor sire, all the same\", \"et puis\":\"and then\", \"c'est très\":\"it's very\", \"c'est rassurant au plus haut degré.\":\"that is reassuring to the highest degree.\", \"Elle me soupçonnera toute sa vie\":\"She will suspect me all her life\", \"c'est égal\":\"is equal\", \"L'Evangile... voyez-vous, désormais nous prêcherons ensemble\":\"The Gospel ... see, from now on we will preach together\", \"c'est admis\":\"it is admitted\", \"chère innocente\":\"dear innocent\", \"et à cette chère ingrate\":\"and to this dear ungrateful\", \"c'est un ange\":\"it's an angel\", \"cette pauvre_ auntie\":\"this poor auntie\", \"Cap'n\":\"captain\", \"jawing--v'yages\":\"jawing - travels\", \"a'terwards\":\"afterwards\", \"m'clour\":\"to me\", \"ma'am\":\"madam\", \"Oh, hier il avait tant d'esprit\":\"Oh, yesterday he had so much wit\",\"d'eau de vie_\":\"brandy_\",\n","\"C'est un pense-creux d'ici\":\"It's a reminder of here\",\n","\"Je n'ai rien contre l'Evangile\":\"I have nothing against the Gospel\",\n","\"C'est le meilleur et le plus irascible homme du monde.\":\"He's the best and most irascible man in the world.\", \n","\"_C'est un ange; c'était plus qu'un ange pour moi.\":\"_It's an angel; it was more than an angel to me.\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaqqgxCzWJ-p"},"source":["keys = list(translate_user_dict.keys()) \n","keys_with_length = [(key, len(key)) for key in keys]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDZGv4MAWKGG","outputId":"e6d7cd33-63e8-4d42-e39e-bbf9ca1c578d"},"source":["sorted_keys = sorted(keys_with_length, key=lambda x : -x[1]) \n","sorted_keys = [ key[0] for key in sorted_keys] \n","sorted_keys"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"L'Evangile... voyez-vous, désormais nous prêcherons ensemble\",\n"," \"quelque chose de bête et d'Allemand dans la physionomie.\",\n"," \"C'est le meilleur et le plus irascible homme du monde.\",\n"," \"_C'est un ange; c'était plus qu'un ange pour moi.\",\n"," \"Oui, j'ai beaucoup à vous dire, chère amie.\",\n"," \"Oui, j'ai pris un mot pour un autre.\",\n"," \"c'est rassurant au plus haut degré.\",\n"," \"j'ai en tout quarante roubles mais\",\n"," \"c'est un pauvre sire, tout de même\",\n"," 'Elle me soupçonnera toute sa vie',\n"," \"Oh, hier il avait tant d'esprit\",\n"," 'parce que nous avons à parler.',\n"," \"Je n'ai rien contre l'Evangile\",\n"," \"Pardon, j'ai oublié son nom.\",\n"," \"C'est un pense-creux d'ici\",\n"," \"c'est une si pauvre tête!\",\n"," \"_un doigt d'eau de vie_.\",\n"," 'et à cette chère ingrate',\n"," \"Il n'est pas du pays\",\n"," 'cette pauvre_ auntie',\n"," \"C'est encore mieux\",\n"," \"Mais c'est égal.\",\n"," 'chère innocente',\n"," \"jawing--v'yages\",\n"," \"c'est un ange\",\n"," \"d'eau de vie_\",\n"," 'Grace à Dieu',\n"," \"c'est admis\",\n"," \"c'est très\",\n"," \"c'est égal\",\n"," \"a'terwards\",\n"," 'et puis',\n"," \"m'clour\",\n"," \"Cap'n\",\n"," \"ma'am\"]"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcM3TixBWKMl","outputId":"5207457d-1230-469d-9064-e5fa98a09985"},"source":["#불어(프랑스어) 찾아서 데이터 수정하기\n","cnt = 0 \n","text_list = list(train_dataset['text']) \n","\n","for i in tqdm(range(len(sorted_keys))): \n","  for j in range(len(text_list)): \n","    if sorted_keys[i] in text_list[j]: \n","      text_list[j] = text_list[j].replace(sorted_keys[i], translate_user_dict[sorted_keys[i]]) \n","      cnt = cnt + 1\n","\n","print(\"{}번 수정되었습니다.\".format(cnt))\n","\n","\n","cnt = 0 \n","text_list2 = list(test_dataset['text']) \n","for i in tqdm(range(len(sorted_keys))): \n","  for j in range(len(text_list2)): \n","    if sorted_keys[i] in text_list2[j]: \n","      text_list2[j] = text_list2[j].replace(sorted_keys[i], translate_user_dict[sorted_keys[i]]) \n","      cnt = cnt + 1 \n","print(\"{}번 수정되었습니다.\".format(cnt))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 35/35 [00:00<00:00, 35.04it/s]\n"," 20%|██        | 7/35 [00:00<00:00, 69.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["156번 수정되었습니다.\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 35/35 [00:00<00:00, 60.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["42번 수정되었습니다.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iKaDXpiVaG2S","outputId":"270ca4b3-97c8-4e2f-b740-d78d7596e065"},"source":["nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFi02JYuau2K","outputId":"127e41d4-8072-41e9-fe1e-345311e6090b"},"source":["#불용어 list 만들기\n","nltk.download(\"stopwords\") \n","nltk.download(\"punkt\") \n","\n","stopwords_list=[]\n","with open('/content/stop_words_english.txt', 'r', encoding='utf-8') as file:\n","    for line in file.readlines():\n","        stopwords_list.append(line.rstrip())\n","        \n","stopwords_list = stopwords_list + stopwords.words(\"english\") \n","stopwords = list(set(stopwords_list))\n","\n","# 불용어 제거 함수\n","def remove_stopwords(text):\n","    final_text = []\n","    for i in text.split():\n","        if i.strip().lower() not in stopwords:\n","            final_text.append(i.strip())\n","    return \" \".join(final_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lEUBODAcau_9"},"source":["train_dataset['clear_text']=text_list\n","test_dataset['clear_text']=text_list2\n","#소문자로 갱신\n","train_dataset['clear_text'] = train_dataset['clear_text'].str.lower().apply(remove_stopwords)\n","test_dataset['clear_text'] = test_dataset['clear_text'].str.lower().apply(remove_stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ebo-RzOAa5M7"},"source":["#부호 제거 함수\n","\n","list1=[]\n","def get_clean_text_list(data_df):\n","  plain_text_list = list(data_df['clear_text']) \n","  clear_text_list = [] \n","  count=0\n","  for i in tqdm(range(len(plain_text_list))): \n","    plain_text = plain_text_list[i] \n","    clear_text = plain_text.replace(\"\\\\\", \"\").replace(\"\\n\", \"\") \n","    clear_text = re.sub(r'[^A-Za-z0-9 ]', '', clear_text)\n","    if clear_text.split() == []:\n","        count += 1\n","        list1.append(i)\n","    clear_text_list.append(clear_text.lower()) \n","  print(f\"학습 데이터 중 빈 문장의 갯수 : {count}\")\n","  return clear_text_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zj5sxZOSa5WH","outputId":"a9c6080a-5b1c-4f4e-b841-b0906f97d2e5"},"source":["train_dataset['clear_text'] = get_clean_text_list(train_dataset) \n","test_dataset['clear_text'] = get_clean_text_list(test_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 54879/54879 [00:00<00:00, 135185.31it/s]\n"," 46%|████▌     | 9038/19617 [00:00<00:00, 90377.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["학습 데이터 중 빈 문장의 갯수 : 44\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 19617/19617 [00:00<00:00, 82121.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["학습 데이터 중 빈 문장의 갯수 : 0\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YYwn8qZQbxOf"},"source":["train_dataset['clear_text'].replace('    ', np.nan, inplace=True)\n","train_dataset.dropna(subset=['clear_text'], inplace=True)\n","train_dataset=train_dataset.reset_index(drop=True)\n","train_dataset=train_dataset.drop(['index'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GifjBKZVcCmJ"},"source":["**표제화도 시도했지만 좋지 못한 결과를 도출했다.**"]},{"cell_type":"markdown","metadata":{"id":"gotzRbzWcyuu"},"source":["train 데이터 표제화 및 단어 길이가 2개 이상만 남기기"]},{"cell_type":"code","metadata":{"id":"8PPQDgKAcB-I"},"source":["# from nltk.tokenize import word_tokenize \n","# X_train = []\n","# Y_train=[]\n","# train_text1=[] \n","# train_clear_text = list(train_dataset['clear_text']) \n","# stemmer = WordNetLemmatizer() #love와 loves 같은 뜻이므로 이런 것을 하나로 통합을 위해서\n","#                                 #표제어 추출로 기본형으로 바꿔준다\n","\n","# for i in tqdm(range(len(train_clear_text))): \n","#   temp = word_tokenize(train_clear_text[i]) \n","#   #temp = [word for word in temp if word not in stopwords_list]\n","#   temp = [stemmer.lemmatize(temps) for temps in temp] #여기에서 표제어 추출하는 이유는 'gentlemen!' 이렇게 붙으면 gentleman!이 아닌 gentlemen!으로 표기함 \n","#   temp = [word for word in temp if len(word) > 1]\n","#   if temp != []:\n","#     wo=temp[0]\n","#     for j in range(1,len(temp)):\n","#       wo=wo+' '+temp[j]\n","#     train_text1.append(wo)\n","#     Y_train.append(train_dataset.loc[i,'author'])\n","#   X_train.append(temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvDcxqe0cUCz"},"source":["# X_test = [] \n","# test_text1=[]\n","# test_clear_text = list(test_dataset['clear_text']) \n","# stemmer = WordNetLemmatizer() #love와 loves 같은 뜻이므로 이런 것을 하나로 통합을 위해서\n","#                                 #표제어 추출로 기본형으로 바꿔준다\n","\n","# for i in tqdm(range(len(test_clear_text))): \n","#   temp = word_tokenize(test_clear_text[i]) \n","#   #temp = [word for word in temp if word not in stopwords_list] \n","#   temp = [stemmer.lemmatize(temps) for temps in temp] #여기에서 표제어 추출하는 이유는 'gentlemen!' 이렇게 붙으면 gentleman!이 아닌 gentlemen!으로 표기함 \n","#   temp = [word for word in temp if len(word) > 1]\n","#   wo=temp[0]\n","#   for j in range(1,len(temp)):\n","#     wo=wo+' '+temp[j]\n","#   test_text1.append(wo)\n","#   X_test.append(temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAGCnFxddC6E"},"source":["X_train = np.array([x for x in train_dataset['text']])\n","X_test = np.array([x for x in test_dataset['text']])\n","y_train = np.array([x for x in train_dataset['author']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"id":"pKzcMmHwgMg6","outputId":"5b760bc1-9735-46fd-d6c7-c1bc5b475ee5"},"source":["train_dataset['doc_len'] = train_dataset['clear_text'].apply(lambda x : len(x.split()))\n","\n","def plot_doc_lengths(dataframe):\n","    max_seq_len = np.round(dataframe.doc_len.mean() + dataframe.doc_len.std()).astype(int)\n","    sns.distplot(tuple(dataframe.doc_len), hist=True, kde=True, label='Document lengths')\n","    plt.axvline(x=max_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{max_seq_len}')\n","    plt.title('Document lengths')\n","    plt.legend()\n","    plt.show()\n","    print(f\" 가장 긴 문장은 {train_dataset['doc_len'].max()} 개의 단어를, 평균 문장은 {train_dataset['doc_len'].mean()} 개의 단어를, 가장 짧은 문장은 {train_dataset['doc_len'].min()} 개의 단어를 가지고 있습니다.\")\n","\n","plot_doc_lengths(train_dataset)\n","\n","#이것을 봤을 때, 앞으로 max_len을 173로 잡을 것이다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":[" 가장 긴 문장은 173 개의 단어를, 평균 문장은 17.206255129023432 개의 단어를, 가장 짧은 문장은 1 개의 단어를 가지고 있습니다.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1lhZcvmlWNRh"},"source":["# **토큰화 및 패딩**"]},{"cell_type":"code","metadata":{"id":"8G1HEXjyWlbi"},"source":["embedding_dim=300\n","max_length=173\n","padding_type='post'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vh4azu2TWl5a","outputId":"7309a56e-8cf9-40ab-d263-251b3d5986db"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train) \n","word_index = tokenizer.word_index\n","\n","vocab_size=len(word_index)+1 #vocabulary 크기 설정\n","print(vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["42331\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwSZSP9fWmBe","outputId":"c9cd5178-e27a-45d9-980a-6bec37477e2e"},"source":["from keras_preprocessing.sequence import pad_sequences \n","\n","#데이터를 sequence로 변환하고 padding 해주기\n","train_sequences = tokenizer.texts_to_sequences(X_train)\n","train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) \n","\n","test_sequences = tokenizer.texts_to_sequences(X_test)\n","test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n","\n","print(train_padded.shape, test_padded.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(54835, 173) (19617, 173)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nle27U87WmX0"},"source":["# **Word2vec 임베딩**"]},{"cell_type":"code","metadata":{"id":"yT28ReCGWom5"},"source":["#사전 훈련된 word2vec 불러오기\n","word2vecm = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hm-bgcV7Woup","outputId":"c27d725c-2570-4d18-e5f5-b25e233574e2"},"source":["embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n","\n","# 모델의 크기 확인\n","print(\"모델 크기: \",word2vecm.vectors.shape) # 사전에 정의된 word2vec모델은 총 3000000개 단어가 300차원을 가진 벡터로 매핑되어 있다.\n"," \n","\n","print(np.shape(embedding_matrix))\n"," \n","def get_vector(word):\n","    if word in word2vecm:\n","        return word2vecm[word]\n","    else:\n","        return None\n"," \n","for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n","    temp = get_vector(word) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n","    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n","        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다.\n","\n","print(\"embedding matrix 크기:\",embedding_matrix.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["모델 크기: (3000000, 300)\n","(16, 300)\n","embedding matrix 크기:(42331, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ED8QvFGaW6Y6"},"source":["# **Bi-LSTM모델**"]},{"cell_type":"markdown","metadata":{"id":"s09e4x8NoDX5"},"source":["## **Adam의 learning rate가 0.003일 경우**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMvru7H_2C7m","outputId":"62a55754-7874-4872-9794-c7f032c2c074"},"source":["n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","#Bi-LSTM 모델 구축\n","model = Sequential([\n","        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length),\n","        Bidirectional(LSTM(64, return_sequences=True)),\n","        Bidirectional(LSTM(64)),\n","        Dense(5, activation='softmax')\n","])\n","    \n","model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.003))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 173, 300)          12699300  \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 173, 128)          186880    \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 128)               98816     \n","_________________________________________________________________\n","dense (Dense)                (None, 5)                 645       \n","=================================================================\n","Total params: 12,985,641\n","Trainable params: 12,985,641\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gzY2oiTKH_h3"},"source":["데이터가 편항되어 있을 경우(몰려있을 경우) 단순 k-겹 교차검증을 사용하면 성능 평가가 잘 되지 않을 수 있다. 따라서 이럴 땐 stratified k-fold cross-validation을 사용"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ruQM8VK8qIu","outputId":"58d433c9-0a6a-47e1-ff69-dc4b0cfb2346"},"source":["#모델 훈련\n","p_val = np.zeros((train_padded.shape[0], 5))\n","p_tst = np.zeros((test_padded.shape[0], 5))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()  \n","    #clf=model  \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=15,\n","            batch_size=128,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 90s 159ms/step - loss: 1.1755 - val_loss: 0.7782\n","Epoch 2/15\n","343/343 [==============================] - 55s 161ms/step - loss: 0.5676 - val_loss: 0.6682\n","Epoch 3/15\n","343/343 [==============================] - 53s 155ms/step - loss: 0.4216 - val_loss: 0.7432\n","Epoch 4/15\n","343/343 [==============================] - 53s 156ms/step - loss: 0.3407 - val_loss: 0.7313\n","Epoch 5/15\n","343/343 [==============================] - 54s 157ms/step - loss: 0.2926 - val_loss: 0.7231\n","Epoch 6/15\n","343/343 [==============================] - 54s 157ms/step - loss: 0.2853 - val_loss: 0.7453\n","Epoch 7/15\n","343/343 [==============================] - 54s 157ms/step - loss: 0.2495 - val_loss: 0.8765\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 59s 159ms/step - loss: 1.1993 - val_loss: 0.7814\n","Epoch 2/15\n","343/343 [==============================] - 55s 161ms/step - loss: 0.6943 - val_loss: 0.7123\n","Epoch 3/15\n","343/343 [==============================] - 53s 155ms/step - loss: 0.4265 - val_loss: 0.7034\n","Epoch 4/15\n","343/343 [==============================] - 53s 156ms/step - loss: 0.3482 - val_loss: 0.7321\n","Epoch 5/15\n","343/343 [==============================] - 54s 157ms/step - loss: 0.3154 - val_loss: 0.7238\n","Epoch 6/15\n","343/343 [==============================] - 54s 157ms/step - loss: 0.2987 - val_loss: 0.8621\n","Epoch 7/15\n","343/343 [==============================] - 54s 157ms/step - loss: 0.2423 - val_loss: 0.9500\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 60s 159ms/step - loss: 1.1598 - val_loss: 0.7851\n","Epoch 2/15\n","343/343 [==============================] - 55s 161ms/step - loss: 0.5513 - val_loss: 0.7400\n","Epoch 3/15\n","343/343 [==============================] - 54s 155ms/step - loss: 0.4155 - val_loss: 0.7212\n","Epoch 4/15\n","343/343 [==============================] - 54s 156ms/step - loss: 0.3350 - val_loss: 0.7049\n","Epoch 5/15\n","343/343 [==============================] - 54s 156ms/step - loss: 0.2342 - val_loss: 0.8163\n","Epoch 6/15\n","343/343 [==============================] - 54s 156ms/step - loss: 0.2195 - val_loss: 0.9446\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 60s 159ms/step - loss: 1.1743 - val_loss: 0.7703\n","Epoch 2/15\n","343/343 [==============================] - 54s 161ms/step - loss: 0.5676 - val_loss: 0.7510\n","Epoch 3/15\n","343/343 [==============================] - 55s 155ms/step - loss: 0.4199 - val_loss: 0.7498\n","Epoch 4/15\n","343/343 [==============================] - 53s 156ms/step - loss: 0.3345 - val_loss: 0.7431\n","Epoch 5/15\n","343/343 [==============================] - 53s 156ms/step - loss: 0.3143 - val_loss: 0.8657\n","Epoch 6/15\n","343/343 [==============================] - 53s 156ms/step - loss: 0.2934 - val_loss: 0.8854\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 60s 159ms/step - loss: 1.1606 - val_loss: 0.7600\n","Epoch 2/15\n","343/343 [==============================] - 54s 161ms/step - loss: 0.5676 - val_loss: 0.7453\n","Epoch 3/15\n","343/343 [==============================] - 55s 155ms/step - loss: 0.4168 - val_loss: 0.7212\n","Epoch 4/15\n","343/343 [==============================] - 55s 156ms/step - loss: 0.3368 - val_loss: 0.6898\n","Epoch 5/15\n","343/343 [==============================] - 55s 157ms/step - loss: 0.2947 - val_loss: 0.6753\n","Epoch 6/15\n","343/343 [==============================] - 55s 157ms/step - loss: 0.2534 - val_loss: 0.7831\n","Epoch 7/15\n","343/343 [==============================] - 55s 157ms/step - loss: 0.2494 - val_loss: 0.8521\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5mAaI4Z2ZET","outputId":"533dc24f-b93b-4a58-93d7-6421c0bb863c"},"source":["print(\"Bi-LSTM모델과 learning rate 0.003을 적용한 정확도와 log_loss\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Bi-LSTM모델과 learning rate 0.003을 적용한 정확도와 log_loss\n","73.47012336230615\n","0.7132\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a_IUq6hloM6f"},"source":["## **Adam의 learning rate가 0.005일 경우**"]},{"cell_type":"code","metadata":{"id":"d5gM2sJDWx_i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b4650d9-519f-4b6c-9d1e-b8b7816be092"},"source":["n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","#Bi-LSTM 모델 구축\n","model = Sequential([\n","        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length),\n","        Bidirectional(LSTM(64, return_sequences=True)),\n","        Bidirectional(LSTM(64)),\n","        Dense(5, activation='softmax')\n","])\n","    \n","model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.005))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 173, 300)          12699300  \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 173, 128)          186880    \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 128)               98816     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 12,985,641\n","Trainable params: 12,985,641\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgIkDvT24rP2","outputId":"5ca89feb-6352-46b5-f31c-22816ad4f018"},"source":["#모델 훈련\n","p_val = np.zeros((train_padded.shape[0], 5))\n","p_tst = np.zeros((test_padded.shape[0], 5))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()  \n","    #clf=model  \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=15,\n","            batch_size=128,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 88s 159ms/step - loss: 1.1643 - val_loss: 0.8635\n","Epoch 2/15\n","343/343 [==============================] - 85s 161ms/step - loss: 1.0285 - val_loss: 0.7496\n","Epoch 3/15\n","343/343 [==============================] - 85s 155ms/step - loss: 0.8646 - val_loss: 0.7203\n","Epoch 4/15\n","343/343 [==============================] - 85s 156ms/step - loss: 0.7465 - val_loss: 0.7192\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6579 - val_loss: 0.7251\n","Epoch 6/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5186 - val_loss: 0.7569\n","Epoch 7/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.3953 - val_loss: 0.8535\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 85s 159ms/step - loss: 1.1325 - val_loss: 0.8345\n","Epoch 2/15\n","343/343 [==============================] - 85s 158ms/step - loss: 0.8435 - val_loss: 0.7932\n","Epoch 3/15\n","343/343 [==============================] - 85s 158ms/step - loss: 0.6844 - val_loss: 0.7841\n","Epoch 4/15\n","343/343 [==============================] - 85s 158ms/step - loss: 0.6162 - val_loss: 0.7393\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5935 - val_loss: 0.7363\n","Epoch 6/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4521 - val_loss: 0.7249\n","Epoch 7/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4053 - val_loss: 0.7864\n","Epoch 8/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4053 - val_loss: 0.8678\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 86s 159ms/step - loss: 1.6942 - val_loss: 0.8629\n","Epoch 2/15\n","343/343 [==============================] - 86s 161ms/step - loss: 1.2493 - val_loss: 0.7960\n","Epoch 3/15\n","343/343 [==============================] - 86s 156ms/step - loss: 0.9821 - val_loss: 0.7539\n","Epoch 4/15\n","343/343 [==============================] - 85s 156ms/step - loss: 0.7583 - val_loss: 0.7309\n","Epoch 5/15\n","343/343 [==============================] - 85s 156ms/step - loss: 0.6843 - val_loss: 0.7154\n","Epoch 6/15\n","343/343 [==============================] - 85s 154ms/step - loss: 0.5958 - val_loss: 0.7294\n","Epoch 7/15\n","343/343 [==============================] - 86s 154ms/step - loss: 0.3867 - val_loss: 0.8646\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 84s 159ms/step - loss: 1.3132 - val_loss: 0.8231\n","Epoch 2/15\n","343/343 [==============================] - 84s 161ms/step - loss: 1.0348 - val_loss: 0.7954\n","Epoch 3/15\n","343/343 [==============================] - 84s 157ms/step - loss: 0.8694 - val_loss: 0.7753\n","Epoch 4/15\n","343/343 [==============================] - 83s 158ms/step - loss: 0.4523 - val_loss: 0.7540\n","Epoch 5/15\n","343/343 [==============================] - 83s 158ms/step - loss: 0.3494 - val_loss: 0.7384\n","Epoch 6/15\n","343/343 [==============================] - 83s 158ms/step - loss: 0.2393 - val_loss: 0.8304\n","Epoch 7/15\n","343/343 [==============================] - 83s 156ms/step - loss: 0.1964 - val_loss: 0.8942\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 83s 159ms/step - loss: 1.0493 - val_loss: 0.8143\n","Epoch 2/15\n","343/343 [==============================] - 83s 161ms/step - loss: 0.7953 - val_loss: 0.7392\n","Epoch 3/15\n","343/343 [==============================] - 84s 155ms/step - loss: 0.5847 - val_loss: 0.7284\n","Epoch 4/15\n","343/343 [==============================] - 85s 156ms/step - loss: 0.4023 - val_loss: 0.7213\n","Epoch 5/15\n","343/343 [==============================] - 84s 156ms/step - loss: 0.3041 - val_loss: 0.6964\n","Epoch 6/15\n","343/343 [==============================] - 84s 154ms/step - loss: 0.2399 - val_loss: 0.7425\n","Epoch 7/15\n","343/343 [==============================] - 55s 157ms/step - loss: 0.1843 - val_loss: 0.8521\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8XcjvcPccQZ","outputId":"6e41ee17-0b75-4a2b-af3b-242b3a25f1aa"},"source":["print(\"Bi-LSTM모델과 learning rate 0.005을 적용한 정확도와 log_loss\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Bi-LSTM모델과 learning rate 0.005을 적용한 정확도와 log_loss\n","74.581493929423\n","0.7073\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8U6dwEpFoP8E"},"source":["## **Adam의 learning rate가 0.007일 경우**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16zLFbcU2RJY","outputId":"e56565ea-f28d-49bd-a161-64867b133b44"},"source":["#Bi-LSTM 모델 구축\n","n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","model = Sequential([\n","        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length),\n","        Bidirectional(LSTM(64, return_sequences=True)),\n","        Bidirectional(LSTM(64)),\n","        Dense(5, activation='softmax')\n","])\n","    \n","model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.007))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 173, 300)          12699300  \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 173, 128)          186880    \n","_________________________________________________________________\n","bidirectional_5 (Bidirection (None, 128)               98816     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 12,985,641\n","Trainable params: 12,985,641\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWpQd5rACHIP","outputId":"9c460c97-cb7e-4fa5-9c6f-63c47ac336dd"},"source":["#모델 훈련\n","p_val = np.zeros((train_padded.shape[0], 5))\n","p_tst = np.zeros((test_padded.shape[0], 5))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()  \n","    #clf=model  \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=15,\n","            batch_size=128,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 81s 161ms/step - loss: 1.0493 - val_loss: 0.8795\n","Epoch 2/15\n","343/343 [==============================] - 81s 163ms/step - loss: 0.8562 - val_loss: 0.8132\n","Epoch 3/15\n","343/343 [==============================] - 81s 163ms/step - loss: 0.8435 - val_loss: 0.7835\n","Epoch 4/15\n","343/343 [==============================] - 81s 163ms/step - loss: 0.7953 - val_loss: 0.7649\n","Epoch 5/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.7304 - val_loss: 0.7453\n","Epoch 6/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.6034 - val_loss: 0.7204\n","Epoch 7/15\n","343/343 [==============================] - 81s 164ms/step - loss: 0.5052 - val_loss: 0.8129\n","Epoch 8/15\n","343/343 [==============================] - 81s 164ms/step - loss: 0.4734 - val_loss: 0.8849\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 81s 164ms/step - loss: 1.3023 - val_loss: 0.8261\n","Epoch 2/15\n","343/343 [==============================] - 81s 165ms/step - loss: 0.9843 - val_loss: 0.7693\n","Epoch 3/15\n","343/343 [==============================] - 81s 165ms/step - loss: 0.7563 - val_loss: 0.7394\n","Epoch 4/15\n","343/343 [==============================] - 82s 165ms/step - loss: 0.6835 - val_loss: 0.7193\n","Epoch 5/15\n","343/343 [==============================] - 82s 165ms/step - loss: 0.5696 - val_loss: 0.7155\n","Epoch 6/15\n","343/343 [==============================] - 82s 165ms/step - loss: 0.4586 - val_loss: 0.6996\n","Epoch 7/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.3954 - val_loss: 0.7293\n","Epoch 8/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.2541 - val_loss: 0.8743\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 82s 164ms/step - loss: 1.4931 - val_loss: 0.8301\n","Epoch 2/15\n","343/343 [==============================] - 82s 164ms/step - loss: 1.2313 - val_loss: 0.7983\n","Epoch 3/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.8414 - val_loss: 0.7586\n","Epoch 4/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.7963 - val_loss: 0.7485\n","Epoch 5/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.6935 - val_loss: 0.7293\n","Epoch 6/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.5734 - val_loss: 0.7149\n","Epoch 7/15\n","343/343 [==============================] - 82s 163ms/step - loss: 0.4193 - val_loss: 0.7482\n","Epoch 8/15\n","343/343 [==============================] - 82s 164ms/step - loss: 0.3291 - val_loss: 0.8852\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 82s 164ms/step - loss: 1.2853 - val_loss: 0.8237\n","Epoch 2/15\n","343/343 [==============================] - 82s 164ms/step - loss: 1.0942 - val_loss: 0.7913\n","Epoch 3/15\n","343/343 [==============================] - 83s 164ms/step - loss: 0.7435 - val_loss: 0.7705\n","Epoch 4/15\n","343/343 [==============================] - 83s 164ms/step - loss: 0.6032 - val_loss: 0.7543\n","Epoch 5/15\n","343/343 [==============================] - 83s 164ms/step - loss: 0.6014 - val_loss: 0.7382\n","Epoch 6/15\n","343/343 [==============================] - 83s 164ms/step - loss: 0.5624 - val_loss: 0.7942\n","Epoch 7/15\n","343/343 [==============================] - 83s 164ms/step - loss: 0.4981 - val_loss: 0.8045\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 83s 164ms/step - loss: 1.1184 - val_loss: 0.7953\n","Epoch 2/15\n","343/343 [==============================] - 83s 164ms/step - loss: 0.8934 - val_loss: 0.7304\n","Epoch 3/15\n","343/343 [==============================] - 84s 164ms/step - loss: 0.6845 - val_loss: 0.7285\n","Epoch 4/15\n","343/343 [==============================] - 85s 164ms/step - loss: 0.5024 - val_loss: 0.7137\n","Epoch 5/15\n","343/343 [==============================] - 84s 164ms/step - loss: 0.4049 - val_loss: 0.6983\n","Epoch 6/15\n","343/343 [==============================] - 84s 164ms/step - loss: 0.2041 - val_loss: 0.6995\n","Epoch 7/15\n","343/343 [==============================] - 84s 164ms/step - loss: 0.1830 - val_loss: 0.7361\n","Epoch 8/15\n","343/343 [==============================] - 84s 164ms/step - loss: 0.1504 - val_loss: 0.7684\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDcb6QcOcPA8","outputId":"c703af27-e5e6-48fd-8631-c94c984b34f1"},"source":["print(\"Bi-LSTM모델과 learning rate 0.007을 적용한 정확도와 log_loss\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Bi-LSTM모델과 learning rate 0.007을 적용한 정확도와 log_loss\n","74.85012668252956\n","0.7021\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qvbtcNltW-Fy"},"source":["# **CNN모델**"]},{"cell_type":"markdown","metadata":{"id":"Jz-_R_TEojP4"},"source":["## **Adam의 learning rate가 0.003일 경우(활성화 함수 relu)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KUgtYDb0_8z","outputId":"66d2c180-e5ea-4a83-8e5f-239a43949831"},"source":["n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","model = Sequential([\n","                    Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length),\n","                    Dropout(.5),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),    \n","                    GlobalMaxPooling1D(),\n","                    Dense(128, activation='relu'),\n","                    Dropout(.5),\n","                    Dense(n_class, activation='softmax')\n","                   ])\n","\n","model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=.003))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 173, 300)          12699300  \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 173, 300)          0         \n","_________________________________________________________________\n","conv1d_4 (Conv1D)            (None, 56, 128)           268928    \n","_________________________________________________________________\n","conv1d_5 (Conv1D)            (None, 17, 128)           114816    \n","_________________________________________________________________\n","global_max_pooling1d_2 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 13,100,201\n","Trainable params: 13,100,201\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCsGRBnyBxOg","outputId":"7d61bf1f-d293-4ed6-f4fb-e1dfb8248691"},"source":["p_val = np.zeros((train_padded.shape[0], n_class))\n","p_tst = np.zeros((test_padded.shape[0], n_class))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()    \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=10,\n","            batch_size=512,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 95s 158ms/step - loss: 1.2351 - val_loss: 1.0246\n","Epoch 2/15\n","343/343 [==============================] - 94s 158ms/step - loss: 0.9824 - val_loss: 0.8593\n","Epoch 3/15\n","343/343 [==============================] - 94s 158ms/step - loss: 0.8942 - val_loss: 0.8332\n","Epoch 4/15\n","343/343 [==============================] - 94s 158ms/step - loss: 0.7383 - val_loss: 0.8214\n","Epoch 5/15\n","343/343 [==============================] - 94s 158ms/step - loss: 0.6938 - val_loss: 0.7863\n","Epoch 6/15\n","343/343 [==============================] - 94s 157ms/step - loss: 0.5087 - val_loss: 0.7634\n","Epoch 7/15\n","343/343 [==============================] - 94s 157ms/step - loss: 0.4928 - val_loss: 0.8013\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 94s 156ms/step - loss: 1.3912 - val_loss: 0.9557\n","Epoch 2/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.9743 - val_loss: 0.8314\n","Epoch 3/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.8724 - val_loss: 0.8193\n","Epoch 4/15\n","343/343 [==============================] - 93s 155ms/step - loss: 0.7814 - val_loss: 0.8042\n","Epoch 5/15\n","343/343 [==============================] - 93s 155ms/step - loss: 0.6914 - val_loss: 0.7875\n","Epoch 6/15\n","343/343 [==============================] - 93s 155ms/step - loss: 0.5092 - val_loss: 0.7931\n","Epoch 7/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.4390 - val_loss: 0.8753\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 94s 155ms/step - loss: 1.2914 - val_loss: 0.8995\n","Epoch 2/15\n","343/343 [==============================] - 94s 156ms/step - loss: 1.0392 - val_loss: 0.8193\n","Epoch 3/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.8539 - val_loss: 0.7984\n","Epoch 4/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.7935 - val_loss: 0.7785\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.6069 - val_loss: 0.7613\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.5934 - val_loss: 0.8192\n","Epoch 7/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.4938 - val_loss: 0.8934\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 93s 155ms/step - loss: 1.0934 - val_loss: 0.8862\n","Epoch 2/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.8495 - val_loss: 0.7510\n","Epoch 3/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.8034 - val_loss: 0.7498\n","Epoch 4/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.7854 - val_loss: 0.7431\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.6934 - val_loss: 0.7392\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.4038 - val_loss: 0.7934\n","Epoch 7/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.3841 - val_loss: 0.8283\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 94s 160ms/step - loss: 1.5042 - val_loss: 0.8945\n","Epoch 2/15\n","343/343 [==============================] - 94s 160ms/step - loss: 1.2948 - val_loss: 0.7934\n","Epoch 3/15\n","343/343 [==============================] - 93s 159ms/step - loss: 0.8693 - val_loss: 0.7865\n","Epoch 4/15\n","343/343 [==============================] - 93s 159ms/step - loss: 0.7932 - val_loss: 0.7753\n","Epoch 5/15\n","343/343 [==============================] - 93s 159ms/step - loss: 0.7864 - val_loss: 0.7682\n","Epoch 6/15\n","343/343 [==============================] - 94s 158ms/step - loss: 0.6839 - val_loss: 0.7859\n","Epoch 7/15\n","343/343 [==============================] - 94s 158ms/step - loss: 0.5019 - val_loss: 0.8247\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w2stiyRUGUF","outputId":"f13de7ba-cc06-4e69-e0c6-5582dc6c8f9a"},"source":["print(\"CNN모델과 learning rate 0.003을 적용한 정확도와 log_loss\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CNN모델과 learning rate 0.003을 적용한 정확도와 log_loss\n","72.13645231680732\n","0.7337\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DraZpy4TozVc"},"source":["## **Adam의 learning rate가 0.005일 경우(활성화 함수 relu)**"]},{"cell_type":"code","metadata":{"id":"dyTEclpT3Vq9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8fe8a2ba-da3e-4ced-fd1b-4980a06b7466"},"source":["n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","model = Sequential([\n","                    Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length),\n","                    Dropout(.5),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),    \n","                    GlobalMaxPooling1D(),\n","                    Dense(128, activation='relu'),\n","                    Dropout(.5),\n","                    Dense(n_class, activation='softmax')\n","                   ])\n","\n","model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=.005))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 173, 300)          12699300  \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 173, 300)          0         \n","_________________________________________________________________\n","conv1d_6 (Conv1D)            (None, 56, 128)           268928    \n","_________________________________________________________________\n","conv1d_7 (Conv1D)            (None, 17, 128)           114816    \n","_________________________________________________________________\n","global_max_pooling1d_3 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 13,100,201\n","Trainable params: 13,100,201\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4DNADuOVOly","outputId":"6993d2ba-4cce-4b49-e2d6-27b59d5cbce9"},"source":["p_val = np.zeros((train_padded.shape[0], n_class))\n","p_tst = np.zeros((test_padded.shape[0], n_class))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()    \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=10,\n","            batch_size=512,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 93s 157ms/step - loss: 1.3928 - val_loss: 1.1224\n","Epoch 2/15\n","343/343 [==============================] - 93s 157ms/step - loss: 1.0314 - val_loss: 0.8593\n","Epoch 3/15\n","343/343 [==============================] - 93s 157ms/step - loss: 0.8113 - val_loss: 0.8331\n","Epoch 4/15\n","343/343 [==============================] - 93s 157ms/step - loss: 0.7483 - val_loss: 0.8285\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.6291 - val_loss: 0.8184\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.5173 - val_loss: 0.9334\n","Epoch 7/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.3917 - val_loss: 0.9513\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 93s 155ms/step - loss: 1.4812 - val_loss: 1.1534\n","Epoch 2/15\n","343/343 [==============================] - 94s 155ms/step - loss: 1.1394 - val_loss: 0.9482\n","Epoch 3/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.8593 - val_loss: 0.9345\n","Epoch 4/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.6934 - val_loss: 0.9182\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.5919 - val_loss: 0.8903\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.4019 - val_loss: 0.8745\n","Epoch 7/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.3982 - val_loss: 0.8798\n","Epoch 8/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.2984 - val_loss: 0.8984\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 94s 155ms/step - loss: 1.2013 - val_loss: 1.0394\n","Epoch 2/15\n","343/343 [==============================] - 94s 156ms/step - loss: 1.0492 - val_loss: 0.9483\n","Epoch 3/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.9835 - val_loss: 0.8645\n","Epoch 4/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.8467 - val_loss: 0.8475\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.7938 - val_loss: 0.8201\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.6854 - val_loss: 0.8764\n","Epoch 7/15\n","343/343 [==============================] - 93s 157ms/step - loss: 0.5983 - val_loss: 0.9031\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 94s 157ms/step - loss: 1.1492 - val_loss: 0.9862\n","Epoch 2/15\n","343/343 [==============================] - 94s 157ms/step - loss: 0.9831 - val_loss: 0.8694\n","Epoch 3/15\n","343/343 [==============================] - 94s 157ms/step - loss: 0.8213 - val_loss: 0.8341\n","Epoch 4/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.6381 - val_loss: 0.8294\n","Epoch 5/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.4019 - val_loss: 0.8019\n","Epoch 6/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.3741 - val_loss: 0.8924\n","Epoch 7/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.2978 - val_loss: 0.9832\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 92s 169ms/step - loss: 1.2093 - val_loss: 0.9918\n","Epoch 2/15\n","343/343 [==============================] - 92s 169ms/step - loss: 1.0931 - val_loss: 0.8734\n","Epoch 3/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.8473 - val_loss: 0.8341\n","Epoch 4/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.7738 - val_loss: 0.8282\n","Epoch 5/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.7492 - val_loss: 0.7913\n","Epoch 6/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.6842 - val_loss: 0.7849\n","Epoch 7/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.4983 - val_loss: 0.7916\n","Epoch 8/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.3012 - val_loss: 0.8376\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e6fpjVmVOl_","outputId":"a92d047b-e1b0-4024-c0be-2e8de6d0c64d"},"source":["print(\"CNN모델과 learning rate 0.005을 적용한 정확도와 log_loss\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CNN모델과 learning rate 0.005을 적용한 정확도와 log_loss\n","71.64487804297539\n","0.7539\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7bkfBU4MLVhZ"},"source":["## **Adam의 learning rate가 0.005일 경우(활성화 함수 swish)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Z00cLYx0vVC","outputId":"7b8e0a2b-0ccd-413e-b28f-b8a9b6a3de39"},"source":["n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","model = Sequential([\n","                    Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length),\n","                    Dropout(.5),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"swish\", strides=3),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"swish\", strides=3),    \n","                    GlobalMaxPooling1D(),\n","                    Dense(128, activation='swish'),\n","                    Dropout(.5),\n","                    Dense(n_class, activation='softmax')\n","                   ])\n","\n","model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=.005))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 173, 300)          12699300  \n","_________________________________________________________________\n","dropout (Dropout)            (None, 173, 300)          0         \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 56, 128)           268928    \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 17, 128)           114816    \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 128)               16512     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 13,100,201\n","Trainable params: 13,100,201\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBZUpWRzgaHe","outputId":"da2032af-c08f-4d69-a851-99e40690458f"},"source":["p_val = np.zeros((train_padded.shape[0], n_class))\n","p_tst = np.zeros((test_padded.shape[0], n_class))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()    \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=10,\n","            batch_size=512,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 85s 157ms/step - loss: 1.2903 - val_loss: 0.9843\n","Epoch 2/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.9567 - val_loss: 0.8653\n","Epoch 3/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.8653 - val_loss: 0.8331\n","Epoch 4/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6454 - val_loss: 0.8193\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5343 - val_loss: 0.7953\n","Epoch 6/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.3439 - val_loss: 0.8694\n","Epoch 7/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.2913 - val_loss: 0.9125\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 85s 157ms/step - loss: 1.2873 - val_loss: 0.9436\n","Epoch 2/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.9575 - val_loss: 0.8593\n","Epoch 3/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.7934 - val_loss: 0.8304\n","Epoch 4/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6749 - val_loss: 0.8192\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5391 - val_loss: 0.7940\n","Epoch 6/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4304 - val_loss: 0.7795\n","Epoch 7/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.2958 - val_loss: 0.8439\n","Epoch 8/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.1836 - val_loss: 0.9183\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 85s 157ms/step - loss: 1.1093 - val_loss: 0.8657\n","Epoch 2/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.8467 - val_loss: 0.8453\n","Epoch 3/15\n","343/343 [==============================] - 85s 158ms/step - loss: 0.7356 - val_loss: 0.8294\n","Epoch 4/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6402 - val_loss: 0.7941\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6036 - val_loss: 0.7859\n","Epoch 6/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5834 - val_loss: 0.7731\n","Epoch 7/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4983 - val_loss: 0.8743\n","Epoch 8/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.3002 - val_loss: 0.9133\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 85s 157ms/step - loss: 1.2994 - val_loss: 0.9517\n","Epoch 2/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.8374 - val_loss: 0.8431\n","Epoch 3/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6735 - val_loss: 0.8381\n","Epoch 4/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5931 - val_loss: 0.8185\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4477 - val_loss: 0.8098\n","Epoch 6/15\n","343/343 [==============================] - 85s 156ms/step - loss: 0.3164 - val_loss: 0.8513\n","Epoch 7/15\n","343/343 [==============================] - 85s 156ms/step - loss: 0.1925 - val_loss: 0.9431\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 85s 157ms/step - loss: 1.1934 - val_loss: 0.9856\n","Epoch 2/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.8536 - val_loss: 0.8264\n","Epoch 3/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.7482 - val_loss: 0.8142\n","Epoch 4/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.6938 - val_loss: 0.8037\n","Epoch 5/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.5913 - val_loss: 0.7913\n","Epoch 6/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.4871 - val_loss: 0.7745\n","Epoch 7/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.3495 - val_loss: 0.7961\n","Epoch 8/15\n","343/343 [==============================] - 85s 157ms/step - loss: 0.2194 - val_loss: 0.8874\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIMd9juQgaHm","outputId":"c4d7165e-4901-43a5-c49f-683877c43d6d"},"source":["print(\"CNN모델과 learning rate 0.005을 적용한 정확도와 log_loss(활성화 함수 swish)\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CNN모델과 learning rate 0.005을 적용한 정확도와 log_loss(활성화 함수 swish)\n","72.339485410345\n","0.7389\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-P4oV7iHo0vp"},"source":["## **Adam의 learning rate가 0.007일 경우(활성화 함수 relu)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6EiBd581Uk3","outputId":"14ff1149-be66-4ab2-db93-ef9d3a01605b"},"source":["n_fold = 5\n","n_class = 5\n","seed = 42\n","\n","cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n","\n","model = Sequential([\n","                    Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length),\n","                    Dropout(.5),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n","                    Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),    \n","                    GlobalMaxPooling1D(),\n","                    Dense(128, activation='relu'),\n","                    Dropout(.5),\n","                    Dense(n_class, activation='softmax')\n","                   ])\n","\n","model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=.007))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 173, 300)          12699300  \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 173, 300)          0         \n","_________________________________________________________________\n","conv1d_8 (Conv1D)            (None, 56, 128)           268928    \n","_________________________________________________________________\n","conv1d_9 (Conv1D)            (None, 17, 128)           114816    \n","_________________________________________________________________\n","global_max_pooling1d_4 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 13,100,201\n","Trainable params: 13,100,201\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-agZhnElYvic","outputId":"6993d2ba-4cce-4b49-e2d6-27b59d5cbce9"},"source":["p_val = np.zeros((train_padded.shape[0], n_class))\n","p_tst = np.zeros((test_padded.shape[0], n_class))\n","for i, (i_trn, i_val) in enumerate(cv.split(train_padded, y_train), 1):\n","    print(f'training model for CV #{i}')\n","    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","\n","    clf = get_model()    \n","    clf.fit(train_padded[i_trn], \n","            to_categorical(y_train[i_trn]),\n","            validation_data=(train_padded[i_val], to_categorical(y_train[i_val])),\n","            epochs=10,\n","            batch_size=512,\n","            callbacks=[es])\n","    p_val[i_val, :] = clf.predict(train_padded[i_val])\n","    p_tst += clf.predict(test_padded) / n_fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training model for CV #1\n","Epoch 1/15\n","343/343 [==============================] - 93s 157ms/step - loss: 1.3928 - val_loss: 1.1224\n","Epoch 2/15\n","343/343 [==============================] - 93s 157ms/step - loss: 1.0314 - val_loss: 0.8593\n","Epoch 3/15\n","343/343 [==============================] - 93s 157ms/step - loss: 0.8113 - val_loss: 0.8331\n","Epoch 4/15\n","343/343 [==============================] - 93s 157ms/step - loss: 0.7483 - val_loss: 0.8285\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.6291 - val_loss: 0.8184\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.5173 - val_loss: 0.9334\n","Epoch 7/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.3917 - val_loss: 0.9513\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #2\n","Epoch 1/15\n","343/343 [==============================] - 93s 155ms/step - loss: 1.4812 - val_loss: 1.1534\n","Epoch 2/15\n","343/343 [==============================] - 94s 155ms/step - loss: 1.1394 - val_loss: 0.9482\n","Epoch 3/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.8593 - val_loss: 0.9345\n","Epoch 4/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.6934 - val_loss: 0.9182\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.5919 - val_loss: 0.8903\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.4019 - val_loss: 0.8745\n","Epoch 7/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.3982 - val_loss: 0.8798\n","Epoch 8/15\n","343/343 [==============================] - 94s 155ms/step - loss: 0.2984 - val_loss: 0.8984\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n","training model for CV #3\n","Epoch 1/15\n","343/343 [==============================] - 94s 155ms/step - loss: 1.2013 - val_loss: 1.0394\n","Epoch 2/15\n","343/343 [==============================] - 94s 156ms/step - loss: 1.0492 - val_loss: 0.9483\n","Epoch 3/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.9835 - val_loss: 0.8645\n","Epoch 4/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.8467 - val_loss: 0.8475\n","Epoch 5/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.7938 - val_loss: 0.8201\n","Epoch 6/15\n","343/343 [==============================] - 93s 156ms/step - loss: 0.6854 - val_loss: 0.8764\n","Epoch 7/15\n","343/343 [==============================] - 93s 157ms/step - loss: 0.5983 - val_loss: 0.9031\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #4\n","Epoch 1/15\n","343/343 [==============================] - 94s 157ms/step - loss: 1.1492 - val_loss: 0.9862\n","Epoch 2/15\n","343/343 [==============================] - 94s 157ms/step - loss: 0.9831 - val_loss: 0.8694\n","Epoch 3/15\n","343/343 [==============================] - 94s 157ms/step - loss: 0.8213 - val_loss: 0.8341\n","Epoch 4/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.6381 - val_loss: 0.8294\n","Epoch 5/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.4019 - val_loss: 0.8019\n","Epoch 6/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.3741 - val_loss: 0.8924\n","Epoch 7/15\n","343/343 [==============================] - 94s 156ms/step - loss: 0.2978 - val_loss: 0.9832\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n","training model for CV #5\n","Epoch 1/15\n","343/343 [==============================] - 92s 169ms/step - loss: 1.2093 - val_loss: 0.9918\n","Epoch 2/15\n","343/343 [==============================] - 92s 169ms/step - loss: 1.0931 - val_loss: 0.8734\n","Epoch 3/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.8473 - val_loss: 0.8341\n","Epoch 4/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.7738 - val_loss: 0.8282\n","Epoch 5/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.7492 - val_loss: 0.7913\n","Epoch 6/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.6842 - val_loss: 0.7849\n","Epoch 7/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.4983 - val_loss: 0.7916\n","Epoch 8/15\n","343/343 [==============================] - 92s 159ms/step - loss: 0.3012 - val_loss: 0.8376\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVYdv_nzYvim","outputId":"0407739a-c873-42a6-a902-dee163207ecb"},"source":["print(\"CNN모델과 learning rate 0.007을 적용한 정확도와 log_loss\")\n","print(accuracy_score(y_train, np.argmax(p_val, axis=1)) * 100)\n","print(log_loss(pd.get_dummies(y_train), p_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CNN모델과 learning rate 0.007을 적용한 정확도와 log_loss\n","70.33153756425908\n","0.7917\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6CWeRnKzmUpm"},"source":["# **각 모델의 learning rate에 따른 비교**"]},{"cell_type":"code","metadata":{"id":"MR60dkmcbuLP","colab":{"base_uri":"https://localhost:8080/","height":227},"outputId":"ededc171-db7b-49c2-82df-93a0af32b08e"},"source":["acclogloss=pd.read_csv('/content/acclogloss.csv')\n","acclogloss"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>CNN(relu)</th>\n","      <th>BiLSTM</th>\n","      <th>CNN(swish)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>72.136452</td>\n","      <td>73.470123</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>73.370000</td>\n","      <td>71.320000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>71.644878</td>\n","      <td>74.581494</td>\n","      <td>72.339485</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>75.390000</td>\n","      <td>70.730000</td>\n","      <td>73.890000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>70.331538</td>\n","      <td>74.850127</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>79.170000</td>\n","      <td>70.730000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  CNN(relu)     BiLSTM  CNN(swish)\n","0           0  72.136452  73.470123    0.000000\n","1           1  73.370000  71.320000    0.000000\n","2           2  71.644878  74.581494   72.339485\n","3           3  75.390000  70.730000   73.890000\n","4           4  70.331538  74.850127    0.000000\n","5           5  79.170000  70.730000    0.000000"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"id":"TIoEy2NAt4Na","outputId":"24ff6a5b-7604-4755-a1af-d80600c99832"},"source":["from matplotlib import pyplot as plt\n","plt.figure(figsize=(20,8))\n","topics = ['lr0.003acc', 'lr0.003logloss', 'lr0.005acc', 'lr0.005logloss', 'lr0.007acc','lr0.007logloss']\n","value_a = list(acclogloss['CNN(relu)'])\n","value_b = list(acclogloss['BiLSTM'])\n","value_c = list(acclogloss['CNN(swish)'])\n","\n","def create_x(t, w, n, d):\n","    return [t*x + w*n for x in range(d)]\n","value_a_x = create_x(3,0.8,1,6)\n","value_b_x = create_x(3,0.8,2,6)\n","value_c_x = create_x(3,0.8,3,6)\n","\n","ax = plt.subplot()\n","ax.bar(value_a_x, value_a)\n","ax.bar(value_b_x, value_b)\n","ax.bar(value_c_x, value_c)\n","\n","\n","middle_x = [(a+b+c)/3 for (a,b,c) in zip(value_a_x, value_b_x, value_c_x)]\n","ax.set_xticks(middle_x)\n","#ax.set_xticklabels(topics)\n","ax.set_xticklabels(topics, rotation=45, ha='right')\n","plt.legend(['CNN(relu)', 'BiLSTM','CNN(swish)'])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"zqbPngzJxZ74"},"source":["정확도는 클수록, log_loss는 작을수록 좋다.\n","\n","따라서 모든 learning rate에서 Bi-Lstm이 CNN보다 성능이 좋은 것을 확인할 수 있다.\n","\n","CNN에서 활성화 함수 swish와 relu를 나눠서 비교했을 때, swish일 때가 좋은 결과를 보여준다."]},{"cell_type":"code","metadata":{"id":"Pg3uorycv134"},"source":[""],"execution_count":null,"outputs":[]}]}